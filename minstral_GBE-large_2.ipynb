{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dad9ba",
   "metadata": {},
   "source": [
    "1 - bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d11aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036dcf3",
   "metadata": {},
   "source": [
    "2 - download dos artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "\n",
    "# 2. Configura o cliente e a busca\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:astro-ph\",\n",
    "    max_results=1000,\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "print(\"Iniciando busca e download...\")\n",
    "\n",
    "for i, result in enumerate(client.results(search)):\n",
    "    pdf_url = result.pdf_url\n",
    "    safe_title = \"\".join(c for c in result.title if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "    filename = f\"pdfs/paper_{i}_{safe_title[:50]}.pdf\"\n",
    "    \n",
    "    print(f\"Baixando: {result.title}...\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(pdf_url)\n",
    "        r.raise_for_status() \n",
    "        \n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o artigo {i}: {e}\")\n",
    "\n",
    "print(\"\\nProcesso finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2ddde",
   "metadata": {},
   "source": [
    "3 - extra√ß√£o de textos dos PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a02c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pdfs\"\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "texts_metadata = []\n",
    "\n",
    "def clean_academic_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "if os.path.exists(path):\n",
    "    files = sorted([f for f in os.listdir(path) if f.endswith(\".pdf\")])\n",
    "    print(f\"Iniciando a leitura de {len(files)} arquivos...\")\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            reader = PdfReader(os.path.join(path, file))\n",
    "            full_text = \"\"\n",
    "            \n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text + \" \"\n",
    "            \n",
    "            cleaned_text = clean_academic_text(full_text)\n",
    "            \n",
    "            if len(cleaned_text) > 100:\n",
    "                \n",
    "                texts_metadata.append({\n",
    "                    \"source\": file,\n",
    "                    \"text\": cleaned_text\n",
    "                })\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Progresso: {i + 1}/{len(files)} arquivos processados.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no arquivo {file}: {e}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, \"extracted_texts.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(texts_metadata, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nExtra√ß√£o conclu√≠da! {len(texts_metadata)} textos salvos em '{output_dir}/extracted_texts.json'\")\n",
    "\n",
    "else:\n",
    "    print(\"A pasta 'pdfs' n√£o foi encontrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff312159",
   "metadata": {},
   "source": [
    "4 - chunckeriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56894c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"processed_data\"\n",
    "output_dir = \"processed_data\"\n",
    "json_input = os.path.join(input_dir, \"extracted_texts.json\")\n",
    "\n",
    "if os.path.exists(json_input):\n",
    "    with open(json_input, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts_metadata = json.load(f)\n",
    "else:\n",
    "    print(\"Erro: extracted_texts.json n√£o encontrado. Rode o c√≥digo de extra√ß√£o primeiro.\")\n",
    "    texts_metadata = []\n",
    "\n",
    "def chunk_text_with_overlap(text, size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), size - overlap):\n",
    "        chunk = \" \".join(words[i : i + size])\n",
    "        chunks.append(chunk)\n",
    "        if i + size >= len(words):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "all_chunks_with_metadata = []\n",
    "\n",
    "print(f\"Iniciando a divis√£o de {len(texts_metadata)} documentos...\")\n",
    "\n",
    "for entry in texts_metadata:\n",
    "    source_name = entry[\"source\"]\n",
    "    text_content = entry[\"text\"]\n",
    "    \n",
    "    doc_chunks = chunk_text_with_overlap(text_content, size=512, overlap=50)\n",
    "    \n",
    "    for chunk in doc_chunks:\n",
    "        \n",
    "        all_chunks_with_metadata.append({\n",
    "            \"source\": source_name,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "output_path = os.path.join(output_dir, \"chunks_data.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks_with_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Total de chunks gerados: {len(all_chunks_with_metadata)}\")\n",
    "print(f\"Dados de chunks salvos em: {output_path}\")\n",
    "\n",
    "if all_chunks_with_metadata:\n",
    "    print(f\"\\nExemplo de metadados do primeiro chunk:\")\n",
    "    print(f\"Fonte: {all_chunks_with_metadata[0]['source']}\")\n",
    "    print(f\"Conte√∫do: {all_chunks_with_metadata[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6dd68",
   "metadata": {},
   "source": [
    "5 - embbending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd394233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o modelo BGE-Large...\n",
      "Iniciando a codifica√ß√£o de 15604 blocos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcaf53c54c14ef982d101d76eeb90c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Carregando o modelo BGE-Large...\")\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "input_path = os.path.join(\"processed_data\", \"chunks_data.json\")\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks_with_metadata = json.load(f)\n",
    "\n",
    "all_chunks_text = [item[\"text\"] for item in chunks_with_metadata]\n",
    "\n",
    "print(f\"Iniciando a codifica√ß√£o de {len(all_chunks_text)} blocos...\")\n",
    "embeddings = model.encode(\n",
    "    all_chunks_text, \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32, \n",
    "    convert_to_numpy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37936bfa",
   "metadata": {},
   "source": [
    "6 - banco vetorial FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff465f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Matriz de embeddings conclu√≠da: (15604, 1024)\n",
      "‚úÖ √çndice FAISS salvo em: processed_data\\vector_index.faiss\n"
     ]
    }
   ],
   "source": [
    "dimension = embeddings.shape[1] \n",
    "index = faiss.IndexFlatL2(dimension) \n",
    "index.add(embeddings.astype('float32')) \n",
    "\n",
    "faiss_path = os.path.join(\"processed_data\", \"vector_index.faiss\")\n",
    "faiss.write_index(index, faiss_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Matriz de embeddings conclu√≠da: {embeddings.shape}\")\n",
    "print(f\"‚úÖ √çndice FAISS salvo em: {faiss_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e1a3a",
   "metadata": {},
   "source": [
    "7 - recupera√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d782ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"processed_data\"\n",
    "faiss_path = os.path.join(input_dir, \"vector_index.faiss\")\n",
    "chunks_path = os.path.join(input_dir, \"chunks_data.json\")\n",
    "\n",
    "print(\"Carregando √≠ndice FAISS e metadados...\")\n",
    "if not os.path.exists(faiss_path) or not os.path.exists(chunks_path):\n",
    "    print(\"Erro: Arquivos de dados processados n√£o encontrados. Rode o Pipeline de Ingest√£o primeiro.\")\n",
    "else:\n",
    "\n",
    "    index = faiss.read_index(faiss_path)\n",
    "    \n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_chunks_with_metadata = json.load(f)\n",
    "    \n",
    "    model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    print(f\"Sistema pronto! Usando {device} para busca.\")\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "    instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "    \n",
    "    q_emb = model.encode([instruction + query]).astype(\"float32\")\n",
    "    \n",
    "    D, I = index.search(q_emb, k)\n",
    "    \n",
    "    retrieved_results = []\n",
    "    for idx in I[0]:\n",
    "\n",
    "        chunk_info = all_chunks_with_metadata[idx]\n",
    "        retrieved_results.append({\n",
    "            \"text\": chunk_info[\"text\"],\n",
    "            \"source\": chunk_info[\"source\"]\n",
    "        })\n",
    "        \n",
    "    return retrieved_results\n",
    "\n",
    "pergunta = \"What are the main observations of gravitational waves in neutron stars?\"\n",
    "contextos = retrieve(pergunta, k=3)\n",
    "\n",
    "print(f\"\\nüîé Pergunta: {pergunta}\")\n",
    "print(f\"üìö {len(contextos)} trechos t√©cnicos encontrados.\\n\")\n",
    "\n",
    "for i, res in enumerate(contextos):\n",
    "    print(f\"üìç [Trecho {i+1}] - Fonte: {res['source']}\")\n",
    "    print(f\"üìÑ Conte√∫do: {res['text'][:400]}...\") \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf738d",
   "metadata": {},
   "source": [
    "8 - minstral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec011f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in globals():\n",
    "    try:\n",
    "        model.to(\"cpu\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 \n",
    ")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "print(\"Carregando Mistral-7B a partir do cache/hub...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    max_memory={0: \"4.8GiB\"}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Mistral-7B carregado com sucesso e pronto para ler o FAISS/JSON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7423d",
   "metadata": {},
   "source": [
    "9 - fun√ß√£o de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff90cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query):\n",
    "    results = retrieve(query, k=4)\n",
    "    \n",
    "    context_text = \"\"\n",
    "    for i, res in enumerate(results):\n",
    "\n",
    "        context_text += f\"[Reference {i+1} from {res['source']}]: {res['text']}\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"<s>[INST] SYSTEM: You are a rigorous scientific validator. Use ONLY the provided SOURCES to answer. \"\n",
    "        f\"If the specific details (equations, constants, or data) are not in the SOURCES, \"\n",
    "        f\"say 'Information not available in dataset'. DO NOT use general knowledge about physics. \"\n",
    "        f\"Be precise about Energy Conditions and Wormhole stability as described in the text.\\n\\n\"\n",
    "        f\"SOURCES:\\n{context_text[:2500]}\\n\\n\"\n",
    "        f\"QUESTION:\\n{query} [/INST]\\n\"\n",
    "        f\"Scientific Analysis based on Sources:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_llm.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "    output_tokens = model_llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1, \n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1, \n",
    "        top_p=0.9, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_tokens = output_tokens[0][input_length:]\n",
    "    clean_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return clean_answer, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3acb6",
   "metadata": {},
   "source": [
    "10 - teste final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"how does the wormhole solution work according to equation 3.57?\"\n",
    "\n",
    "print(f\"üöÄ [1/2] Gerindo mem√≥ria e preparando busca...\")\n",
    "\n",
    "if 'model' in globals():\n",
    "    model.to(\"cpu\")\n",
    "    device_embedding = \"cpu\" \n",
    "else:\n",
    "    device_embedding = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(f\"üß† [2/2] Consultando Mistral-7B (4-bit) na GPU...\")\n",
    "    \n",
    "    resposta, results = ask(pergunta)\n",
    "\n",
    "    print(\"\\n\" + \"‚ïê\"*60)\n",
    "    print(f\"üåå RESPOSTA CIENT√çFICA:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(resposta)\n",
    "    print(\"‚ïê\"*60)\n",
    "\n",
    "    print(\"\\nüìö FONTES UTILIZADAS (Ancoragem):\")\n",
    "    for i, res in enumerate(results[:3]):\n",
    "\n",
    "        print(f\"   [{i+1}] Fonte: {res['source']}\")\n",
    "        print(f\"       Trecho: \\\"{res['text'][:150]}...\\\"\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\n‚ùå ERRO DE MEM√ìRIA: A GPU esgotou. Tente reduzir o 'k' na fun√ß√£o retrieve.\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3451e21",
   "metadata": {},
   "source": [
    "11 - metricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10673e5",
   "metadata": {},
   "source": [
    "11.1 - indice de fidelidade semantica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464cd9d",
   "metadata": {},
   "source": [
    "A m√©trica de Fidelidade Sem√¢ntica serve para medir o grau de honestidade intelectual do seu assistente em rela√ß√£o aos documentos fornecidos, funcionando como um detector de mentiras que impede a intelig√™ncia artificial de ignorar os artigos cient√≠ficos para inventar respostas baseadas apenas em seu treinamento pr√©vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = pergunta \n",
    "answer_text = resposta\n",
    "contexto_consolidado = \" \".join([res['text'] for res in results])\n",
    "\n",
    "print(\"Calculando vetores de fidelidade...\")\n",
    "ans_emb = model.encode([answer_text])\n",
    "ctx_emb = model.encode([contexto_consolidado])\n",
    "\n",
    "fidelidade_score = cosine_similarity(ans_emb, ctx_emb)[0][0]\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üî¨ RELAT√ìRIO DE VERIFICA√á√ÉO CIENT√çFICA\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìä Score de Fidelidade: {fidelidade_score:.4f}\")\n",
    "\n",
    "if fidelidade_score > 0.82:\n",
    "    status = \"‚úÖ EXCELENTE: Resposta totalmente ancorada nos artigos.\"\n",
    "elif fidelidade_score > 0.65:\n",
    "    status = \"‚ö†Ô∏è ATEN√á√ÉO: A resposta √© relevante, mas pode conter infer√™ncias externas ao dataset.\"\n",
    "else:\n",
    "    status = \"‚ùå ALERTA: Poss√≠vel alucina√ß√£o ou resposta baseada apenas no conhecimento pr√©vio do modelo.\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(\"‚ïê\"*50)\n",
    "\n",
    "q_emb = model.encode([query_text])\n",
    "relevancia_pergunta = cosine_similarity(ans_emb, q_emb)[0][0]\n",
    "print(f\"üéØ Relev√¢ncia da Resposta para a Pergunta: {relevancia_pergunta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8ec2f",
   "metadata": {},
   "source": [
    "11.2 - metrica de alucina√ß√£o negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f082d15",
   "metadata": {},
   "source": [
    "a m√©trica de Alucina√ß√£o Negativa, ou Gap Analysis, √© projetada para identificar se o modelo est√° de fato extraindo informa√ß√µes √∫teis dos textos ou se est√° apenas realizando um \"enrolation\" t√©cnico ao parafrasear a sua pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_emb = model.encode([resposta])\n",
    "query_emb = model.encode([pergunta])\n",
    "\n",
    "sim_pergunta = cosine_similarity(ans_emb, query_emb)[0][0]\n",
    "\n",
    "hallucination_gap = fidelidade_score - sim_pergunta\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä M√âTRICA DE AGREGA√á√ÉO T√âCNICA (Gap Analysis)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìà Fidelidade (Resposta-Contexto): {fidelidade_score:.4f}\")\n",
    "print(f\"üîÑ Repeti√ß√£o (Resposta-Pergunta):  {sim_pergunta:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"üéØ GAP DE CONHECIMENTO: {hallucination_gap:.4f}\")\n",
    "\n",
    "if hallucination_gap > 0.10:\n",
    "    status = \"‚úÖ SEGURO: O modelo extraiu e sintetizou informa√ß√µes novas dos artigos.\"\n",
    "elif hallucination_gap > 0.0:\n",
    "    status = \"üü° NEUTRO: O modelo seguiu a pergunta, mas a contribui√ß√£o dos artigos foi moderada.\"\n",
    "elif hallucination_gap > -0.10:\n",
    "    status = \"‚ö†Ô∏è ALERTA: A resposta est√° muito presa ao texto da pergunta (Parafraseamento).\"\n",
    "else:\n",
    "    status = \"‚ùå CR√çTICO: Poss√≠vel alucina√ß√£o ou resposta gen√©rica ignorando os fatos dos PDFs.\"\n",
    "\n",
    "print(f\"\\nStatus: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb1128",
   "metadata": {},
   "source": [
    "11.3 - m√©trica de densidade de informa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8eaa9d",
   "metadata": {},
   "source": [
    "a Densidade de Informa√ß√£o foca na qualidade estrutural e na sofistica√ß√£o do texto produzido, sendo fundamental para evitar que o modelo entre em ciclos de repeti√ß√£o ou utilize uma linguagem excessivamente gen√©rica e simplista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77101e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_limpas = re.findall(r'\\w+', resposta.lower())\n",
    "vocabulario_unico = set(palavras_limpas)\n",
    "\n",
    "total_palavras = len(palavras_limpas)\n",
    "total_unicas = len(vocabulario_unico)\n",
    "densidade = total_unicas / total_palavras if total_palavras > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä M√âTRICA DE RIQUEZA LEXICAL\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total de palavras na resposta: {total_palavras}\")\n",
    "print(f\"Vocabul√°rio √∫nico:             {total_unicas}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"√çndice de Densidade:           {densidade:.4f}\")\n",
    "\n",
    "if densidade > 0.65:\n",
    "    status = \"‚úÖ EXCELENTE: Texto denso, t√©cnico e sem repeti√ß√µes desnecess√°rias.\"\n",
    "elif densidade > 0.45:\n",
    "    status = \"üü° NORMAL: Fluidez adequada para uma explica√ß√£o cient√≠fica.\"\n",
    "else:\n",
    "    status = \"‚ö†Ô∏è REPETITIVO: O modelo pode estar 'preso' em um loop. Considere subir o 'repetition_penalty'.\"\n",
    "\n",
    "print(f\"\\nStatus: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42e242",
   "metadata": {},
   "source": [
    "11.4 - m√©trica de precis√£o de recupera√ß√£o(analise sobre o BGE-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aae405",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_texto = [res['text'] for res in results]\n",
    "\n",
    "query_embedding = model.encode([pergunta])\n",
    "\n",
    "context_embeddings = model.encode(contextos_texto)\n",
    "\n",
    "similaridades = cosine_similarity(query_embedding, context_embeddings)[0]\n",
    "\n",
    "mean_similarity = np.mean(similaridades)\n",
    "max_similarity = np.max(similaridades)\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä PERFORMANCE DO RETRIEVER (BGE-LARGE)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Similaridade M√°xima (Top 1):  {max_similarity:.4f}\")\n",
    "print(f\"Similaridade M√©dia (Top {len(similaridades)}):   {mean_similarity:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if max_similarity > 0.75:\n",
    "    status = \"‚úÖ ALTA RELEV√ÇNCIA: O BGE encontrou trechos muito espec√≠ficos nos artigos.\"\n",
    "elif max_similarity > 0.55:\n",
    "    status = \"üü° RELEV√ÇNCIA M√âDIA: O conte√∫do √© correlato, mas pode ser gen√©rico.\"\n",
    "else:\n",
    "    status = \"‚ùå BAIXA RELEV√ÇNCIA: O banco de 1000 PDFs pode n√£o conter a resposta exata.\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
