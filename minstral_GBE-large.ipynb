{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dad9ba",
   "metadata": {},
   "source": [
    "1 - bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d11aa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kyush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036dcf3",
   "metadata": {},
   "source": [
    "2 - download dos artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "\n",
    "# 2. Configura o cliente e a busca\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:astro-ph\",\n",
    "    max_results=1000,\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "print(\"Iniciando busca e download...\")\n",
    "\n",
    "# 3. Processa os resultados e faz o download\n",
    "for i, result in enumerate(client.results(search)):\n",
    "    pdf_url = result.pdf_url\n",
    "    # Limpa o t√≠tulo para evitar caracteres inv√°lidos no nome do arquivo\n",
    "    safe_title = \"\".join(c for c in result.title if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "    filename = f\"pdfs/paper_{i}_{safe_title[:50]}.pdf\"\n",
    "    \n",
    "    print(f\"Baixando: {result.title}...\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(pdf_url)\n",
    "        r.raise_for_status() # Verifica se o download deu certo\n",
    "        \n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar o artigo {i}: {e}\")\n",
    "\n",
    "print(\"\\nProcesso finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2ddde",
   "metadata": {},
   "source": [
    "3 - extra√ß√£o de textos dos PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a02c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a leitura de 997 arquivos...\n",
      "Progresso: 50/997 arquivos processados.\n",
      "Progresso: 100/997 arquivos processados.\n",
      "Progresso: 150/997 arquivos processados.\n",
      "Progresso: 200/997 arquivos processados.\n",
      "Progresso: 250/997 arquivos processados.\n",
      "Progresso: 300/997 arquivos processados.\n",
      "Progresso: 350/997 arquivos processados.\n",
      "Progresso: 400/997 arquivos processados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Illegal character in Name Object (b'/FPEFAA+Times New Roman \\xcf\\xee\\xeb\\xf3\\xe6\\xe8\\xf0\\xed\\xfb\\xe9')\n",
      "Illegal character in Name Object (b'/FPEFAA+Times New Roman \\xcf\\xee\\xeb\\xf3\\xe6\\xe8\\xf0\\xed\\xfb\\xe9')\n",
      "Illegal character in Name Object (b'/FPEFAF+Times New Roman \\xcf\\xee\\xeb\\xf3\\xe6\\xe8\\xf0\\xed\\xfb\\xe9 \\xca\\xf3\\xf0\\xf1\\xe8\\xe2')\n",
      "Illegal character in Name Object (b'/FPEFAF+Times New Roman \\xcf\\xee\\xeb\\xf3\\xe6\\xe8\\xf0\\xed\\xfb\\xe9 \\xca\\xf3\\xf0\\xf1\\xe8\\xe2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso: 450/997 arquivos processados.\n",
      "Progresso: 500/997 arquivos processados.\n",
      "Progresso: 550/997 arquivos processados.\n",
      "Progresso: 600/997 arquivos processados.\n",
      "Progresso: 650/997 arquivos processados.\n",
      "Progresso: 700/997 arquivos processados.\n",
      "Progresso: 750/997 arquivos processados.\n",
      "Progresso: 800/997 arquivos processados.\n",
      "Progresso: 850/997 arquivos processados.\n",
      "Progresso: 900/997 arquivos processados.\n",
      "Progresso: 950/997 arquivos processados.\n",
      "\n",
      "Total de textos extra√≠dos com sucesso: 997\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "path = \"pdfs\"\n",
    "\n",
    "# Fun√ß√£o simples para limpar ru√≠dos comuns de PDFs acad√™micos\n",
    "def clean_academic_text(text):\n",
    "    # Remove excesso de espa√ßos e quebras de linha estranhas\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove refer√™ncias comuns de rodap√© ou URLs (opcional, mas ajuda o GPT-2)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "if os.path.exists(path):\n",
    "    # Lista e filtra apenas arquivos PDF\n",
    "    files = sorted([f for f in os.listdir(path) if f.endswith(\".pdf\")])\n",
    "    \n",
    "    # Se voc√™ quiser testar com os 1000, garantimos que ele pegue todos ou o limite dispon√≠vel\n",
    "    print(f\"Iniciando a leitura de {len(files)} arquivos...\")\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            reader = PdfReader(os.path.join(path, file))\n",
    "            full_text = \"\"\n",
    "            \n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text + \" \"\n",
    "            \n",
    "            # Aplicamos a limpeza antes de salvar\n",
    "            cleaned_text = clean_academic_text(full_text)\n",
    "            \n",
    "            if len(cleaned_text) > 100: # Ignora arquivos quase vazios ou corrompidos\n",
    "                texts.append(cleaned_text)\n",
    "            \n",
    "            # Feedback a cada 50 arquivos para n√£o inundar o console do Jupyter\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Progresso: {i + 1}/{len(files)} arquivos processados.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no arquivo {file}: {e}\")\n",
    "else:\n",
    "    print(\"A pasta 'pdfs' n√£o foi encontrada.\")\n",
    "\n",
    "print(f\"\\nTotal de textos extra√≠dos com sucesso: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff312159",
   "metadata": {},
   "source": [
    "4 - chunckeriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c56894c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a divis√£o de 997 documentos...\n",
      "Total de documentos: 997\n",
      "Total de chunks: 15604\n",
      "Exemplo do primeiro chunk (100 caracteres): arXiv:0808.3195v1 [astro-ph] 23 Aug 2008Version October 24, 2018 Oblique Ion Two-Stream Instability ...\n"
     ]
    }
   ],
   "source": [
    "# Ajustamos o tamanho para 512, que √© o limite padr√£o do BGE-Large\n",
    "def chunk_text_with_overlap(text, size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), size - overlap):\n",
    "        # Cria o bloco com o tamanho definido\n",
    "        chunk = \" \".join(words[i : i + size])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Para economizar mem√≥ria com 1000 artigos, paramos se o bloco for muito curto\n",
    "        if i + size >= len(words):\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Criamos as listas para armazenar os blocos e o mapeamento de qual arquivo eles vieram\n",
    "all_chunks = []\n",
    "metadata = []\n",
    "\n",
    "print(f\"Iniciando a divis√£o de {len(texts)} documentos...\")\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    # Geramos os blocos para cada texto\n",
    "    doc_chunks = chunk_text_with_overlap(text, size=512, overlap=50)\n",
    "    \n",
    "    for chunk in doc_chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        # Guardamos o √≠ndice do documento original para saber a fonte depois\n",
    "        metadata.append({\"doc_index\": i})\n",
    "\n",
    "print(f\"Total de documentos: {len(texts)}\")\n",
    "print(f\"Total de chunks: {len(all_chunks)}\")\n",
    "print(f\"Exemplo do primeiro chunk (100 caracteres): {all_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6dd68",
   "metadata": {},
   "source": [
    "5 - embbending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd394233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o modelo BGE-Large... Isso pode levar um minuto.\n",
      "Usando dispositivo: cuda\n",
      "Iniciando a codifica√ß√£o de 15604 blocos. Aguarde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133bcceaf253430db4574b9633ebc0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formato da matriz de embeddings: (15604, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 1. Carrega o modelo BGE-Large (Aproximadamente 1.3GB)\n",
    "# Este modelo √© top-tier em rankings de busca sem√¢ntica\n",
    "print(\"Carregando o modelo BGE-Large... Isso pode levar um minuto.\")\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# 2. Configura√ß√£o de Hardware\n",
    "# Se voc√™ tiver uma GPU NVIDIA, o c√≥digo a usar√° automaticamente para acelerar os 1000 artigos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# 3. Transforma os chunks em vetores num√©ricos\n",
    "# Usamos 'all_chunks' que j√° possui o tratamento de sobreposi√ß√£o (overlap)\n",
    "print(f\"Iniciando a codifica√ß√£o de {len(all_chunks)} blocos. Aguarde...\")\n",
    "embeddings = model.encode(\n",
    "    all_chunks, \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32, # Ajuste o batch_size se houver erro de mem√≥ria (OOM)\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFormato da matriz de embeddings: {embeddings.shape}\")\n",
    "# O shape ser√° (n√∫mero_de_chunks, 1024)\n",
    "# Note que a dimens√£o subiu de 384 para 1024, muito mais informa√ß√£o por vetor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37936bfa",
   "metadata": {},
   "source": [
    "6 - banco vetorial FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff465f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ √çndice FAISS configurado para 1024 dimens√µes.\n",
      "üìä Total de blocos cient√≠ficos indexados: 15604\n",
      "üöÄ O √≠ndice est√° pronto para buscas de alta precis√£o.\n"
     ]
    }
   ],
   "source": [
    "# 1. Obt√©m a dimens√£o dos vetores dinamicamente\n",
    "# Agora ser√° 1024 (proveniente do BGE-Large)\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "# 2. Cria o √≠ndice usando dist√¢ncia Euclidiana (L2)\n",
    "# O IndexFlatL2 √© o padr√£o ouro para precis√£o absoluta (exaustivo)\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# 3. Convers√£o e Adi√ß√£o dos dados\n",
    "# Certificamos que os dados est√£o em float32 para otimiza√ß√£o do FAISS\n",
    "embeddings_f32 = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Adiciona ao √≠ndice\n",
    "index.add(embeddings_f32)\n",
    "\n",
    "print(f\"‚úÖ √çndice FAISS configurado para {dim} dimens√µes.\")\n",
    "print(f\"üìä Total de blocos cient√≠ficos indexados: {index.ntotal}\")\n",
    "\n",
    "# Dica: Verificando se o √≠ndice est√° treinado (FlatL2 n√£o precisa de treino, mas √© bom checar)\n",
    "if index.is_trained:\n",
    "    print(\"üöÄ O √≠ndice est√° pronto para buscas de alta precis√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e1a3a",
   "metadata": {},
   "source": [
    "7 - recupera√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d782ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Pergunta: What are the main observations of gravitational waves in neutron stars?\n",
      "üìö 5 trechos encontrados nos 1000 artigos.\n",
      "\n",
      "üìç [Trecho 1] - Origin√°rio do PDF √≠ndice: 734\n",
      "üìÑ Conte√∫do: and crust. However, these eÔ¨Äects are know n to be very strong [89], so that the inequality (290) should be taken with a grain of salt. 12.5 Gravitational wave asteroseismology The development of gravitational wave detectors like LIGO [ 66], VIRGO [207], TAMA300 [300] and GEO600 [298] is opening up a new window of astronomical obser vations. With a central density on the order of ‚àº1015g cm‚àí3, neutr...\n",
      "--------------------------------------------------\n",
      "üìç [Trecho 2] - Origin√°rio do PDF √≠ndice: 529\n",
      "üìÑ Conte√∫do: arXiv:0808.4002v3 [gr-qc] 11 Mar 2009Gravitational-Wave Extraction from Neutron-Star Oscilla tions: comparing linear and nonlinear techniques. Luca Baiotti,1,2Sebastiano Bernuzzi,3Giovanni Corvino,2,3Roberto De Pietri,3and Alessandro Nagar4,5,6 1Graduate School of Arts and Sciences, University of Tokyo, K omaba, Meguro-ku, Tokyo, 153-8902, Japan 2Max-Planck-Institut f¬® ur Gravitationsphysik, Alber...\n",
      "--------------------------------------------------\n",
      "üìç [Trecho 3] - Origin√°rio do PDF √≠ndice: 103\n",
      "üìÑ Conte√∫do: a physical model for nonlinear saturation in newborn neutron stars. We use one triplet of modes: then= 3,m= 2 r-mode, and two near-resonant inertial modes that couple to it. Nonlinear eÔ¨Äects become impor- tant when the r-mode amplitude grows above its para- metric instability threshold. This threshold provides a physical cutoÔ¨Ä to the r-mode instability by energy trans- fer to other inertial modes ...\n",
      "--------------------------------------------------\n",
      "üìç [Trecho 4] - Origin√°rio do PDF √≠ndice: 983\n",
      "üìÑ Conte√∫do: arXiv:gr-qc/9908027v2 11 Aug 1999Fully general relativistic simulation of coalescing binar y neutron stars: Preparatory tests Masaru Shibata Department of Physics, University of Illinois at Urbana-Ch ampaign, Urbana, IL 61801, USA and Department of Earth and Space Science, Graduate School of Sc ience, Osaka University, Toyonaka, Osaka 560-0043, Japan We present our Ô¨Årst successful numerical result...\n",
      "--------------------------------------------------\n",
      "üìç [Trecho 5] - Origin√°rio do PDF √≠ndice: 529\n",
      "üìÑ Conte√∫do: ones because the star is nonrotating and modes are degenerate in m. V. CONCLUSIONS We have compared various gravitational-wave‚Äì extraction methods that are nowadays very popular in numerical-relativity simulations: (i) the Abrahams- Price [65] technique based on the gauge-invariant Regge-Wheeler-Zerilli-Moncrief perturbation theory of a Schwarzschild space-time; (ii) the extraction method based on...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k=5):\n",
    "    # 1. Transforma a pergunta usando o BGE-Large\n",
    "    # Importante: O BGE recomenda adicionar uma instru√ß√£o curta na query para melhor busca\n",
    "    instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "    q_emb = model.encode([instruction + query]).astype(\"float32\")\n",
    "    \n",
    "    # 2. Busca no FAISS\n",
    "    # Aumentamos a precis√£o com a busca vetorial de 1024 dimens√µes\n",
    "    D, I = index.search(q_emb, k)\n",
    "    \n",
    "    # 3. Retorna os textos e a origem (metadata)\n",
    "    retrieved_chunks = []\n",
    "    for idx in I[0]:\n",
    "        # Recuperamos o texto do chunk\n",
    "        content = all_chunks[idx]\n",
    "        # Recuperamos o √≠ndice do PDF original (metadata que salvamos na etapa 2)\n",
    "        source_id = metadata[idx]['doc_index']\n",
    "        retrieved_chunks.append({\"text\": content, \"source\": source_id})\n",
    "        \n",
    "    return retrieved_chunks\n",
    "\n",
    "# --- Teste de Recupera√ß√£o ---\n",
    "pergunta = \"What are the main observations of gravitational waves in neutron stars?\"\n",
    "contextos = retrieve(pergunta, k=5)\n",
    "\n",
    "print(f\"üîé Pergunta: {pergunta}\")\n",
    "print(f\"üìö {len(contextos)} trechos encontrados nos 1000 artigos.\\n\")\n",
    "\n",
    "for i, res in enumerate(contextos):\n",
    "    print(f\"üìç [Trecho {i+1}] - Origin√°rio do PDF √≠ndice: {res['source']}\")\n",
    "    print(f\"üìÑ Conte√∫do: {res['text'][:400]}...\") \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf738d",
   "metadata": {},
   "source": [
    "8 - minstral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ec011f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando Mistral-7B com otimiza√ß√£o para 6GB VRAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638d78f450cf4261a7716a65efbd0f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Liberar Mem√≥ria do Passo Anterior\n",
    "# Movemos o modelo de embedding para a CPU para abrir espa√ßo na GPU\n",
    "if 'model' in globals():\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 2. Configura√ß√£o de Quantiza√ß√£o 4-bit (NF4)\n",
    "# Essencial para comprimir o modelo de 15GB para ~4.5GB\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 \n",
    ")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "print(\"Carregando Mistral-7B com otimiza√ß√£o para 6GB VRAM...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # Limita o uso para sobrar VRAM para o sistema operacional\n",
    "    max_memory={0: \"4.8GiB\"} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7423d",
   "metadata": {},
   "source": [
    "9 - fun√ß√£o de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9ff90cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query):\n",
    "    # 1. Recupera√ß√£o\n",
    "    results = retrieve(query, k=4)\n",
    "    \n",
    "    # 2. Organiza√ß√£o do contexto (Melhorado para indicar fontes ao modelo)\n",
    "    context_text = \"\"\n",
    "    for i, res in enumerate(results):\n",
    "        context_text += f\"[Reference {i+1}]: {res['text']}\\n\\n\"\n",
    "\n",
    "    # 3. Prompt Refinado (Adicionado restri√ß√µes de precis√£o e densidade)\n",
    "    # Note que pedimos explicitamente por termos t√©cnicos e rela√ß√µes de escala.\n",
    "    prompt = (\n",
    "        f\"<s>[INST] SYSTEM: You are a rigorous scientific validator. Use ONLY the provided SOURCES to answer. \"\n",
    "        f\"If the specific details (equations, constants, or data) are not in the SOURCES, \"\n",
    "        f\"say 'Information not available in dataset'. DO NOT use general knowledge about physics. \"\n",
    "        f\"Be precise about Energy Conditions and Wormhole stability as described in the text.\\n\\n\"\n",
    "        f\"SOURCES:\\n{context_text[:2000]}\\n\\n\"\n",
    "        f\"QUESTION:\\n{query} [/INST]\\n\"\n",
    "        f\"Scientific Analysis based on Sources:\"\n",
    "    )\n",
    "\n",
    "    # 4. Tokeniza√ß√£o e Gera√ß√£o (Hiperpar√¢metros equilibrados)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "    output_tokens = model_llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        # Reduzido de 0.4 para 0.1: Para f√≠sica, queremos o token mais prov√°vel (precis√£o).\n",
    "        temperature=0.1, \n",
    "        do_sample=True, # Mant√©m a gera√ß√£o determin√≠stica\n",
    "        # Reduzido de 1.4 para 1.1: Evita termos inventados e preserva o vocabul√°rio t√©cnico.\n",
    "        repetition_penalty=1.1, \n",
    "        top_p=0.9, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # 5. Retorno Limpo\n",
    "    generated_tokens = output_tokens[0][input_length:]\n",
    "    clean_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return clean_answer, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3acb6",
   "metadata": {},
   "source": [
    "10 - teste final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b18e84c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ [1/2] Iniciando busca sem√¢ntica...\n",
      "üß† [2/2] Consultando Mistral-7B (4-bit) na GPU...\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üåå RESPOSTA CIENT√çFICA:\n",
      "------------------------------------------------------------\n",
      "From the provided sources, we can see that a wormhole solution is presented in equation (3.57) for degree (Œ∏) equal to 9, which gives us the wormhole solution:\n",
      "\n",
      "ds¬≤ = r(r+r‚ÇÄ)¬≤dt¬≤ + 3r‚ÇÄr‚Å¥ dr¬≤ + (r+r‚ÇÄ)‚Å¥ r¬≤dŒ∏¬≤\n",
      "\n",
      "This solution describes a wormhole spacetime where the critical points lie on the Y-axis. However, it's important to note that this wormhole solution is specific to the given degree value (Œ∏ = 9).\n",
      "\n",
      "Regarding energy conditions and wormhole stability, the sources do not provide explicit information about these aspects for the wormhole solution. They focus more on discussing the behavior of different solutions in the phase plane and their asymptotic behavior.\n",
      "\n",
      "For a wormhole to be physically viable, it should satisfy the null energy condition (NEC) and the weak energy condition (WEC). These conditions ensure that the energy density and pressure of any matter distribution within the wormhole are non-negative. Unfortunately, the sources do not provide enough detail to verify whether the wormhole solution (3.57) satisfies these conditions.\n",
      "\n",
      "In addition, wormhole stability requires that the throat radius remains larger than the Schwarzschild radius to prevent the formation of event horizons. Again, the sources do not provide information about the stability of the wormhole solution (3.57).\n",
      "\n",
      "In summary, while the sources provide a mathematical representation of a wormhole solution, they do not offer sufficient information about its energy conditions or stability. Further analysis would be required to determine if this wormhole solution is physically viable.\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "üìö FONTES DE APOIO (TOP 3):\n",
      "  [1] PDF ID: 428 | Trecho: \"X= tanh ¬Ω=2 for the analytic case ( ¬∞=1). This integrates to the metric ds2=¬°¬µ 1 +1 r¬∂2 dt2+ dr2+r2d¬≠2 I I; (3.55) which is not a wormhole, but a ¬∞at ...\"\n",
      "  [2] PDF ID: 904 | Trecho: \"into H4, that is, some kind of structure such that a large volume is contained in a small area; for this will allow the volume te rm in (28) to domina...\"\n",
      "  [3] PDF ID: 428 | Trecho: \"in the area function of the 4-D BBH metric. Such a wormhole region in the black hole geometry is a key potential di¬Æerence from the black holes of sta...\"\n"
     ]
    }
   ],
   "source": [
    "# 1. Defini√ß√£o da pergunta\n",
    "pergunta = \"how the wormhole\"\n",
    "\n",
    "print(f\"üöÄ [1/2] Iniciando busca sem√¢ntica...\")\n",
    "\n",
    "# --- MANIPULA√á√ÉO DE MEM√ìRIA (VITAL PARA 6GB) ---\n",
    "# Movemos o modelo BGE para a CPU para liberar ~1.5GB para o Mistral\n",
    "if 'model' in globals():\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# -----------------------------------------------\n",
    "\n",
    "# 2. Execu√ß√£o da busca e gera√ß√£o\n",
    "# A fun√ß√£o ask chamar√° o retrieve (que usar√° o model na CPU, o que √© r√°pido para 1 query)\n",
    "# E usar√° o model_llm que j√° est√° na GPU\n",
    "try:\n",
    "    print(f\"üß† [2/2] Consultando Mistral-7B (4-bit) na GPU...\")\n",
    "    resposta, results = ask(pergunta)\n",
    "\n",
    "    print(\"\\n\" + \"‚ïê\"*60)\n",
    "    print(f\"üåå RESPOSTA CIENT√çFICA:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(resposta)\n",
    "    print(\"‚ïê\"*60)\n",
    "\n",
    "    print(\"\\nüìö FONTES DE APOIO (TOP 3):\")\n",
    "    for i, res in enumerate(results[:3]):\n",
    "        print(f\"  [{i+1}] PDF ID: {res['source']} | Trecho: \\\"{res['text'][:150]}...\\\"\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\n‚ùå ERRO DE MEM√ìRIA: A GPU esgotou. Tentando limpar e reduzir o contexto...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Tente rodar novamente diminuindo o k na fun√ß√£o retrieve para 2 ou 3.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3451e21",
   "metadata": {},
   "source": [
    "11 - metricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10673e5",
   "metadata": {},
   "source": [
    "11.1 - indice de fidelidade semantica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464cd9d",
   "metadata": {},
   "source": [
    "A m√©trica de Fidelidade Sem√¢ntica serve para medir o grau de honestidade intelectual do seu assistente em rela√ß√£o aos documentos fornecidos, funcionando como um detector de mentiras que impede a intelig√™ncia artificial de ignorar os artigos cient√≠ficos para inventar respostas baseadas apenas em seu treinamento pr√©vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7a8f9193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando vetores de fidelidade...\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üî¨ RELAT√ìRIO DE VERIFICA√á√ÉO CIENT√çFICA\n",
      "--------------------------------------------------\n",
      "üìä Score de Fidelidade: 0.8322\n",
      "Status: ‚úÖ EXCELENTE: Resposta totalmente ancorada nos artigos.\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üéØ Relev√¢ncia da Resposta para a Pergunta: 0.7145\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepara√ß√£o dos dados para valida√ß√£o\n",
    "# Usamos a resposta gerada pelo Mistral e os contextos que o BGE-Large encontrou\n",
    "query_text = pergunta \n",
    "answer_text = resposta\n",
    "# Unimos os textos dos chunks recuperados para formar a base de compara√ß√£o\n",
    "contexto_consolidado = \" \".join([res['text'] for res in results])\n",
    "\n",
    "# 2. Gerar Embeddings para compara√ß√£o\n",
    "# Usamos o mesmo modelo BGE-Large (que j√° est√° carregado) para garantir a consist√™ncia\n",
    "print(\"Calculando vetores de fidelidade...\")\n",
    "ans_emb = model.encode([answer_text])\n",
    "ctx_emb = model.encode([contexto_consolidado])\n",
    "\n",
    "# 3. Calcular Similaridade de Cosseno (Fidelidade Sem√¢ntica)\n",
    "# Mede o quanto a resposta \"flutua\" dentro do espa√ßo sem√¢ntico dos artigos\n",
    "fidelidade_score = cosine_similarity(ans_emb, ctx_emb)[0][0]\n",
    "\n",
    "# 4. Exibi√ß√£o da An√°lise de Qualidade\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üî¨ RELAT√ìRIO DE VERIFICA√á√ÉO CIENT√çFICA\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìä Score de Fidelidade: {fidelidade_score:.4f}\")\n",
    "\n",
    "# Diagn√≥stico refinado para modelos de alta performance como o Mistral\n",
    "if fidelidade_score > 0.82:\n",
    "    status = \"‚úÖ EXCELENTE: Resposta totalmente ancorada nos artigos.\"\n",
    "elif fidelidade_score > 0.65:\n",
    "    status = \"‚ö†Ô∏è ATEN√á√ÉO: A resposta √© relevante, mas pode conter infer√™ncias externas ao dataset.\"\n",
    "else:\n",
    "    status = \"‚ùå ALERTA: Poss√≠vel alucina√ß√£o ou resposta baseada apenas no conhecimento pr√©vio do modelo.\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(\"‚ïê\"*50)\n",
    "\n",
    "# Opcional: Mostrar a dist√¢ncia entre a pergunta original e a resposta\n",
    "q_emb = model.encode([query_text])\n",
    "relevancia_pergunta = cosine_similarity(ans_emb, q_emb)[0][0]\n",
    "print(f\"üéØ Relev√¢ncia da Resposta para a Pergunta: {relevancia_pergunta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8ec2f",
   "metadata": {},
   "source": [
    "11.2 - metrica de alucina√ß√£o negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f082d15",
   "metadata": {},
   "source": [
    "a m√©trica de Alucina√ß√£o Negativa, ou Gap Analysis, √© projetada para identificar se o modelo est√° de fato extraindo informa√ß√µes √∫teis dos textos ou se est√° apenas realizando um \"enrolation\" t√©cnico ao parafrasear a sua pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "22dd7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìä M√âTRICA DE AGREGA√á√ÉO T√âCNICA (Gap Analysis)\n",
      "--------------------------------------------------\n",
      "üìà Fidelidade (Resposta-Contexto): 0.8322\n",
      "üîÑ Repeti√ß√£o (Resposta-Pergunta):  0.7145\n",
      "------------------------------\n",
      "üéØ GAP DE CONHECIMENTO: 0.1177\n",
      "\n",
      "Status: ‚úÖ SEGURO: O modelo extraiu e sintetizou informa√ß√µes novas dos artigos.\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepara√ß√£o dos dados para compara√ß√£o\n",
    "# Usamos a resposta gerada e a pergunta feita para medir a sobreposi√ß√£o\n",
    "ans_emb = model.encode([resposta])\n",
    "query_emb = model.encode([pergunta])\n",
    "\n",
    "# 2. C√°lculo da Similaridade entre Resposta e Pergunta\n",
    "# Se for muito alto, o modelo pode estar apenas \"enrolando\" sem trazer dados dos PDFs\n",
    "sim_pergunta = cosine_similarity(ans_emb, query_emb)[0][0]\n",
    "\n",
    "# 3. C√°lculo do GAP de Alucina√ß√£o / Agrega√ß√£o de Valor\n",
    "# Comparamos o quanto a resposta se aproxima dos artigos (fidelidade_score) \n",
    "# versus o quanto ela se aproxima da pergunta (sim_pergunta)\n",
    "hallucination_gap = fidelidade_score - sim_pergunta\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä M√âTRICA DE AGREGA√á√ÉO T√âCNICA (Gap Analysis)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üìà Fidelidade (Resposta-Contexto): {fidelidade_score:.4f}\")\n",
    "print(f\"üîÑ Repeti√ß√£o (Resposta-Pergunta):  {sim_pergunta:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"üéØ GAP DE CONHECIMENTO: {hallucination_gap:.4f}\")\n",
    "\n",
    "# 4. Diagn√≥stico do Desempenho do RAG\n",
    "if hallucination_gap > 0.10:\n",
    "    status = \"‚úÖ SEGURO: O modelo extraiu e sintetizou informa√ß√µes novas dos artigos.\"\n",
    "elif hallucination_gap > 0.0:\n",
    "    status = \"üü° NEUTRO: O modelo seguiu a pergunta, mas a contribui√ß√£o dos artigos foi moderada.\"\n",
    "elif hallucination_gap > -0.10:\n",
    "    status = \"‚ö†Ô∏è ALERTA: A resposta est√° muito presa ao texto da pergunta (Parafraseamento).\"\n",
    "else:\n",
    "    status = \"‚ùå CR√çTICO: Poss√≠vel alucina√ß√£o ou resposta gen√©rica ignorando os fatos dos PDFs.\"\n",
    "\n",
    "print(f\"\\nStatus: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb1128",
   "metadata": {},
   "source": [
    "11.3 - m√©trica de densidade de informa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8eaa9d",
   "metadata": {},
   "source": [
    "a Densidade de Informa√ß√£o foca na qualidade estrutural e na sofistica√ß√£o do texto produzido, sendo fundamental para evitar que o modelo entre em ciclos de repeti√ß√£o ou utilize uma linguagem excessivamente gen√©rica e simplista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "77101e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìä M√âTRICA DE RIQUEZA LEXICAL\n",
      "--------------------------------------------------\n",
      "Total de palavras na resposta: 248\n",
      "Vocabul√°rio √∫nico:             130\n",
      "------------------------------\n",
      "√çndice de Densidade:           0.5242\n",
      "\n",
      "Status: üü° NORMAL: Fluidez adequada para uma explica√ß√£o cient√≠fica.\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokeniza√ß√£o e Limpeza\n",
    "# Transformamos a resposta em uma lista de palavras, removendo pontua√ß√£o\n",
    "palavras_limpas = re.findall(r'\\w+', resposta.lower())\n",
    "vocabulario_unico = set(palavras_limpas)\n",
    "\n",
    "# 2. C√°lculo da Densidade (Diversidade Lexical)\n",
    "# Mede a riqueza do vocabul√°rio gerado pelo Mistral\n",
    "total_palavras = len(palavras_limpas)\n",
    "total_unicas = len(vocabulario_unico)\n",
    "densidade = total_unicas / total_palavras if total_palavras > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä M√âTRICA DE RIQUEZA LEXICAL\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total de palavras na resposta: {total_palavras}\")\n",
    "print(f\"Vocabul√°rio √∫nico:             {total_unicas}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"√çndice de Densidade:           {densidade:.4f}\")\n",
    "\n",
    "# 3. Diagn√≥stico de Qualidade Textual\n",
    "# Em textos cient√≠ficos, densidade alta indica uso de termos precisos (sem \"encher lingui√ßa\")\n",
    "if densidade > 0.65:\n",
    "    status = \"‚úÖ EXCELENTE: Texto denso, t√©cnico e sem repeti√ß√µes desnecess√°rias.\"\n",
    "elif densidade > 0.45:\n",
    "    status = \"üü° NORMAL: Fluidez adequada para uma explica√ß√£o cient√≠fica.\"\n",
    "else:\n",
    "    status = \"‚ö†Ô∏è REPETITIVO: O modelo pode estar 'preso' em um loop. Considere subir o 'repetition_penalty'.\"\n",
    "\n",
    "print(f\"\\nStatus: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42e242",
   "metadata": {},
   "source": [
    "11.4 - m√©trica de precis√£o de recupera√ß√£o(analise sobre o BGE-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1aae405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìä PERFORMANCE DO RETRIEVER (BGE-LARGE)\n",
      "--------------------------------------------------\n",
      "Similaridade M√°xima (Top 1):  0.8045\n",
      "Similaridade M√©dia (Top 5):   0.7864\n",
      "------------------------------\n",
      "Status: ‚úÖ ALTA RELEV√ÇNCIA: O BGE encontrou trechos muito espec√≠ficos nos artigos.\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
     ]
    }
   ],
   "source": [
    "# 1. Ajuste: Extrair apenas os textos da lista de dicion√°rios 'results'\n",
    "# Isso garante que o modelo receba apenas strings, n√£o metadados.\n",
    "contextos_texto = [res['text'] for res in results]\n",
    "\n",
    "# 2. Obter o embedding da pergunta original\n",
    "query_embedding = model.encode([pergunta])\n",
    "\n",
    "# 3. Obter embeddings dos contextos recuperados para valida√ß√£o\n",
    "# Usamos o BGE que j√° est√° na mem√≥ria (ou CPU)\n",
    "context_embeddings = model.encode(contextos_texto)\n",
    "\n",
    "# 4. C√°lculo das m√©tricas de busca (Similaridade de Cosseno)\n",
    "similaridades = cosine_similarity(query_embedding, context_embeddings)[0]\n",
    "\n",
    "mean_similarity = np.mean(similaridades)\n",
    "max_similarity = np.max(similaridades)\n",
    "\n",
    "# 5. Exibi√ß√£o da Performance do Retriever\n",
    "print(\"\\n\" + \"‚ïê\"*50)\n",
    "print(f\"üìä PERFORMANCE DO RETRIEVER (BGE-LARGE)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Similaridade M√°xima (Top 1):  {max_similarity:.4f}\")\n",
    "print(f\"Similaridade M√©dia (Top {len(similaridades)}):   {mean_similarity:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 6. Diagn√≥stico de Qualidade da Recupera√ß√£o\n",
    "# Com o Mistral, se o Max for baixo, a chance de alucina√ß√£o sobe muito.\n",
    "if max_similarity > 0.75:\n",
    "    status = \"‚úÖ ALTA RELEV√ÇNCIA: O BGE encontrou trechos muito espec√≠ficos nos artigos.\"\n",
    "elif max_similarity > 0.55:\n",
    "    status = \"üü° RELEV√ÇNCIA M√âDIA: O conte√∫do √© correlato, mas pode ser gen√©rico.\"\n",
    "else:\n",
    "    status = \"‚ùå BAIXA RELEV√ÇNCIA: O banco de 1000 PDFs pode n√£o conter a resposta exata.\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(\"‚ïê\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
